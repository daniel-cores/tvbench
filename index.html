<!DOCTYPE html>
<html>

<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0G5SGQ503H"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0G5SGQ503H');
</script>

  <meta charset="utf-8">
  <link rel="icon" type="image/svg+xml" href="./favicon.svg"/>
  <meta name="description" content="TVBench: Redesigning Video-Language Evaluation">
  <meta name="keywords" content="TVBench: Video-Language Evaluation, temporal understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TVBench: Redesigning Video-Language Evaluation</title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 style="font-size: 43px" class="title is-1 publication-title">TVBench: Redesigning Video-Language Evaluation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://daniel-cores.github.io/">Daniel Cores*</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://mdorkenwald.com">Michael Dorkenwald*</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://persoal.citius.usc.es/manuel.mucientes/">Manuel Mucientes</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.ceessnoek.info/">Cees G. M. Snoek</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://yukimasano.github.io/">Yuki M. Asano</a><sup>3</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Santiago de Compostela</span>
              <span class="author-block"><sup>2</sup>University of Amsterdam</span>
              <span class="author-block"><sup>3</sup>University of Technology Nuremberg</span>
            </div>

            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block">ArXiv</span>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/daniel-cores/tvbench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/FunAILab/TVBench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span>ðŸ¤— Huggingface</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- </section> -->

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container" id="container-single-video">
        <div id="single-video-container" class="single-video-container" style="display: flex; justify-content: center;">
          <div class="item item-demo-video">
            <video style="width: 900px;" poster="" id="demo-video-1" autoplay muted controls playsinline height="100%">
              <source src="./static/images/tvbench.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Overview. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              Large language models have demonstrated impressive performance when integrated with vision models, even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. 
              In our paper, we show that the currently most used video-language benchmarks, such as MVBench, can be solved without requiring much temporal reasoning and propose as a solution a new benchmark, TVBench, providing an effective evaluation tool for current video-language models:
            </p>
            
            <ul>
              <li>
                MVBench contains unlikely candidate answers easily dismissed from a single frame.<br> 
                &rarr; We provide only temporally challenging candidate answers, requiring models to leverage temporal information to answer correctly.
              </li>
              <li>
                MVBench contains QA pairs with obvious solutions due to LLM biased generation.<br> 
                &rarr; To address this, we generate questions using text templates, preventing grammatical issues that could be exploited by text-only models.
              </li>
              <li>
                MVBench contains QA pairs that can be solved by solely relying on prior world knowledge.<br>
                &rarr; We design questions that can be answered purely from the video content, without relying on prior world knowledge.
              </li>
            </ul>
            
            <p>
              In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues, while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. 
              As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.
            </p>
            

          </div>
        </div>
      </div>
    <!-- Image Section Below Overview -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <figure>
              <img src="./static/images/fig1.png" alt="Diagram showing TVBench architecture" style="width:100%;max-width:800px;">
            </figure>
          </div>
        </div>

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Does Time Matter?</h2>
        <div class="content has-text-justified">
          <figure>
            <img src="./static/images/tab_Static_bias.png" alt="Diagram showing TVBench architecture" style="width:100%;max-width:800px;">
          </figure>
          <p>
            Video benchmarks must define tasks that cannot be solved using solely spatial information to effectively evaluate a model's temporal understanding. In video multiple-choice question answering (MCQA), questions should not be answerable using spatial details from a single random frame or shuffled frames. If temporal understanding is not required, the benchmark only assesses spatial information, which we define as spatial bias.
          </p>
          
          <p>
            We analyze this spatial bias in MVBench using various state-of-the-art image and video-language models across four tasks: i) scene transition, ii) fine-grained pose, iii) episodic reasoning, and iv) fine-grained action. Image-language models perform unexpectedly well on these tasks, often matching video-language models despite the expectation of temporal reasoning. For example, in the <i>Fine-grained Action</i> task, the image-based GPT-4o model scores 49%, slightly better than the state-of-the-art video model Tarsier at 48.5%.
          </p>
          
          <p>
            Shuffling videos has minimal impact on video-language models, further indicating a lack of temporal dependency. Across all 20 MVBench tasks, GPT-4o achieves an average accuracy of 47.8%, 20.5% above random baseline, highlighting spatial bias. Both VideoChat2 and Tarsier exhibit high consistency between answers from original and shuffled videos, with VideoChat2 providing the same answer 91% of the time and Tarsier 82%. These findings suggest that spatial bias affects the entire MVBench dataset.
          </p>
          

          <figure>
            <img src="./static/images/static_bias.png" alt="Diagram showing TVBench architecture" style="width:100%;max-width:800px;">
          </figure>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

        <!-- Paper video. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Does Vision Matter?</h2>
            <div class="content has-text-justified">
              <figure>
                <img src="./static/images/tab_text_bias.png" alt="Diagram showing TVBench architecture" style="width:100%;max-width:800px;">
              </figure>
              <p>
                Video benchmarks must ensure that questions cannot be answered purely through common sense reasoning. Modern LLMs possess strong reasoning skills, which can exploit the question and candidate sets in MCQA video-language evaluation benchmarks, introducing substantial textual bias. This allows models to answer questions without actually leveraging video content.
              </p>
              
              <p>
                We evaluated this textual bias in MVBench by testing text-only LLMs. Results show that LLMs can eliminate incompatible candidates easily, often matching video-language models' performance. For instance, Llama 3 achieves 44.5% on the <i>Action Count</i> task, closely trailing Tarsier at 46.5%. Across all 20 tasks, Llama 3 averages 38.1%, significantly outperforming the random baseline of 27.3%. We identified three key sources of this bias:
              </p>
              
              <ul>
                <li><strong>Bias from LLM-based QA generation:</strong> Many tasks in MVBench use QA pairs generated by LLMs like ChatGPT, leading to unrealistic candidates or poorly phrased questions that can be solved without visual input. For example, in the <i>Action Antonym</i> task, answers are often unrealistic or irrelevant, allowing text-only models to easily find the correct answer.</li>
              
                <li><strong>Bias from unbalanced QA sets:</strong> Unbalanced candidate sets, such as the <i>Action Count</i> task, where '3' is the correct answer for 45% of the questions, skew the evaluation. Text-only models, like GPT-4o, capitalize on this by overpredicting common answers, yielding results close to video models.</li>
              
                <li><strong>Overreliance on world knowledge:</strong> LLMs can leverage prior world knowledge to guess answers without visual reasoning, even when the questions are well-designed.</li>
              </ul>
              <figure>
                <img src="./static/images/text_bias.png" alt="Diagram showing TVBench architecture" style="width:100%;max-width:800px;">
              </figure>
            </div>
          </div>
        </div>
        <!--/ Paper video. -->

            <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Open-ended QA to the rescue?</h2>
        <div class="content has-text-justified">
          <figure>
            <img src="./static/images/Screenshot 2024-10-15 at 09.21.28.png" alt="Diagram showing TVBench architecture" style="width:80%;max-width:800px;">
          </figure>
          <p>
            Unlike multiple-choice question answering (MCQA), open-ended question answering removes the reliance on predefined candidates, potentially solving the above mentioned issues. However, it introduces new challenges. Following Maaz et al. (2023), LLMs like GPT-3.5 are used to evaluate open-ended question-answers, relying on private APIs which can be unreliable. The evaluation model scores responses based on their alignment with the question and ground-truth answer.
          </p>
          
          <p>
            We analyzed the impact of using different evaluators, GPT-3.5 and Llama3-70B, on three open-ended datasets. The results varied significantly, with discrepancies over 20 points, especially benefiting text-only and single-image models. Llama3 often rated incorrect predictions higher, highlighting the risk of shared biases between the prediction and evaluation models.
          </p>
          
          <p>
            Moreover, open-ended QA does not fully resolve MCQA's problems. Text-only models can guess answers solely from the question, showing strong performance even without temporal understanding. For example, GPT-4o achieves accuracy rates close to video-language models, and shuffling video frames barely affects results, indicating that temporal understanding is not crucial for these tasks.
          </p>
          
          
          <p>
            In conclusion, open-ended benchmarks remain unreliable due to the use of biased LLMs as evaluators. These benchmarks also suffer from spatial and textual biases, making them unsuitable for evaluating video-language models, while relying on costly, non-reproducible private APIs.
          </p>
          
          
          <figure>
            <img src="./static/images/oeqa_eval.png" alt="Diagram showing TVBench architecture" style="width:100%;max-width:800px;">
          </figure>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

        <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TVBench: Temporal Video-Language Benchmark</h2>
        <div class="content has-text-justified">
          <figure>
            <img src="./static/images/tv_bench_principle.png" alt="Diagram showing TVBench architecture" style="width:100%;max-width:800px;">
          </figure>
          <p>
            We propose TVBench, a new benchmark designed to evaluate temporal understanding in video QA. Using a multiple-choice approach, TVBench addresses issues with open-ended VQA evaluations. Its principles focus on overcoming the problems outlined in existing video MCQA benchmarks, and can be applied to any video QA benchmark to enhance temporal complexity. 
          </p>
          
          <h4>Developing a Temporal Video-Language Benchmark</h4>
          
          <strong>1. Defining hard answer candidates:</strong>
          <p>
            To ensure temporal constraints are essential for answering questions correctly, we designed questions with temporal challenging candidates. The figure above illustrates our solution to Problem 1, where hard candidates cannot be dismissed without temporal understanding. Tasks such as Action Count, Object Shuffle, Action Localization, and Scene Transition require temporal reasoning, ensuring single-frame models and shuffled video inputs perform at random levels.
          </p>
          
          <strong>2. Reducing overly informative questions:</strong>
          <p>
            We use templates to avoid text-bias and ensure questions do not give away the answer. Tasks like Action Count, Object Count, and Moving Direction use a balanced candidate set, where each candidate appears equally as the correct answer. This reduces textual bias, requiring models to rely on visual understanding.
          </p>
          
          <strong>3. Omitting questions requiring prior knowledge:</strong>
          <p>
            To mitigate Problem 2, we eliminate tasks that rely on world knowledge, such as Episodic Reasoning from MVBench. TVBench focuses solely on visual information, removing any reliance on common knowledge to ensure evaluations test temporal reasoning and video understanding.
          </p>
          <figure>
            <img src="./static/images/barplot.png" alt="Diagram showing TVBench architecture" style="width:100%;max-width:800px;">
          </figure>

          <h4>Evaluating TVBench</h4>
          <p>
            The figure above summarizes the results of TVBench and contrasts them to MVBench.</p>
          
          <h4>Does Time Matter?</h4>
          <p>
            On TVBench, single-image models perform at random levels, indicating that a random frame is insufficient for accurate answers. For example, GPT-4o outperforms random chance by only 2.5% on TVBench, compared to a 20.5% improvement on MVBench. Shuffling videos has little effect on video models' performance on MVBench but significantly degrades accuracy on TVBench, showing the importance of temporal context. Reversing videos further worsens performance on TVBench, with models like Tarsier-7B and Tarsier-34B scoring below random levels, illustrating their reliance on correct temporal order.
          </p>
          
          <h4>Does Vision Matter?</h4>
          <p>
            Text-only models perform at random levels on TVBench, indicating our solutions to Problem 2 are effective. For instance, Llama 3 performs only 1.4% above random chance on TVBench, compared to a 10.8% improvement on MVBench. This demonstrates that LLMs cannot rely on the question and answer candidates or prior knowledge, making visual information crucial for solving TVBench.
          </p>
          <h4>Overview of TVBench</h4>
          <figure>
            <img src="./static/images/overview.png" alt="Diagram showing TVBench architecture" style="width:100%;max-width:800px;">
          </figure>
          Above is a list of all tasks and the corresponding dataset used for TVBench. Below are random example question-answer videos from our benchmark. 






          <figure>
            <img src="./static/images/figs_apendix_tvbench/action_antonym_S001C001P003R001A008_rgb.jpg" alt="action_antonym_S001C001P003R001A008_rgb" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_antonym_S005C001P021R001A021_rgb.jpg" alt="action_antonym_S005C001P021R001A021_rgb" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_antonym_S006C003P023R001A014_rgb.jpg" alt="action_antonym_S006C003P023R001A014_rgb" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_count_video_145.jpg" alt="action_count_video_145" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_count_video_308.jpg" alt="action_count_video_308" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_count_video_329.jpg" alt="action_count_video_329" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_localization_6IL0C.jpg" alt="action_localization_6IL0C" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_localization_8CCEV.jpg" alt="action_localization_8CCEV" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_localization_V1WN7.jpg" alt="action_localization_V1WN7" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_sequence_1HGEX.jpg" alt="action_sequence_1HGEX" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/action_sequence_B7LO8.jpg" alt="action_sequence_B7LO8" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/count_video_10023.jpg" alt="count_video_10023" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/count_video_10081.jpg" alt="count_video_10081" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/count_video_10347.jpg" alt="count_video_10347" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/moving_direction_video_10040.jpg" alt="moving_direction_video_10040" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/moving_direction_video_10068.jpg" alt="moving_direction_video_10068" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/moving_direction_video_10106.jpg" alt="moving_direction_video_10106" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/object_shuffle_video_441.jpg" alt="object_shuffle_video_441" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/object_shuffle_video_482.jpg" alt="object_shuffle_video_482" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/object_shuffle_video_526.jpg" alt="object_shuffle_video_526" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/scene_transition_Top008_00190.jpg" alt="scene_transition_Top008_00190" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/scene_transition_Top014_03245.jpg" alt="scene_transition_Top014_03245" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/scene_transition_Top023_05940.jpg" alt="scene_transition_Top023_05940" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/unexpected_action_H_H_46_0625_0746.jpg" alt="unexpected_action_H_H_46_0625_0746" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/unexpected_action_H_H_121_3397_3512.jpg" alt="unexpected_action_H_H_121_3397_3512" style="width:100%;max-width:800px;">

            <img src="./static/images/figs_apendix_tvbench/unexpected_action_M_C_654_0217_0421.jpg" alt="unexpected_action_M_C_654_0217_0421" style="width:100%;max-width:800px;">
          </figure>
          
        </div>
      </div>
    </div>
    <!--/ Paper video. -->


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@InProceedings{cores2025tvbench,
  author    = {Daniel Cores and Michael Dorkenwald and Manuel Mucientes and Cees G. M. Snoek and Yuki M. Asano},
  title     = {{TVBench}: Redesigning Video-Language Evaluation},
  journal   = {ArXiv},
  year      = {2024}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This webpage is adapted under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
``
